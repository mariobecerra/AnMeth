---
title: "Examen 1 (Métodos Analíticos)"
author: "Mario Becerra 124362"
date: "16/03/2015"
output: html_document
---

#Parte 1: correos de Enron, similitud y minhashing.

En este ejemplo, mediante minhashing/Locality-Sensitive Hashing, construiremos una aplicación para devolver rápidamente correos similares a uno dado, en el sentido de que contienen palabras similares.

Utilizaremos los datos de correo de Enron de https://archive.ics.uci.edu/ml/datasets/Bag+of+Words. 

```{r, message=FALSE, cache=TRUE}
library(dplyr)
enron <- read.table('./Data/enron/docword.enron.txt', skip=3, header=FALSE, sep=' ')
names(enron) <- c('doc','word_id','n')
head(enron)
vocab <- read.table('./Data/enron/vocab.enron.txt', header=FALSE)
vocab$word_id <- 1:nrow(vocab)
head(vocab)
enron <- arrange(enron, word_id)
```

Usaremos similitud de Jaccard basada en el modelo bag of words para documentos, es decir, solo en función de las palabras que contienen: la similitud entre el documento A y B es el número de palabras en común dividido entre el número total de palabras que ocurren en los dos documentos. Aquí hay una implementación simple:

```{r, cache=TRUE}
jaccard <- function(doc1, doc2){
  a <- length(union(doc1$word_id, doc2$word_id))
  c <- length(intersect(doc1$word_id, doc2$word_id))
  c/a
}
jaccard(filter(enron, doc==1), filter(enron, doc==2))
```

Tenemos una colección de `r length(unique(enron$doc))` mails cargados en forma de lista larga.

El objetivo ahora es construir una matriz de firmas de minhashing para esta colección y utilizar esta matriz de firmas para encontrar mails similares al 900 (con más de 50% de similitud de Jaccard).

Definimos primero la función hash. Tendremos 200 hashes en total, así que la matriz de firmas tendrá dimensión $200 \times `r length(unique(enron$doc))`$.

```{r, eval=FALSE}
set.seed(2805)
hash.lista <- lapply(1:200, function(i){
    primo <- length(unique(enron$word_id))
    a <- sample(1:(primo-1), 1)
    b <- sample(1:(primo-1), 1)
    function(x){
        ((a*(x-1) + b) %% primo)  + 1
    }
})
```

La siguiente función (*update_mat*), escrita en C++ para mejorar el desempeño, actualiza la matriz de firmas de acuerdo al siguiente algoritmo:

Supongamos que tenemos $h_1,...,h_m$ funciones hash.

Definimos inicialmente la matriz sig de firmas como $sig(i,c)=\infty$.

* Para cada renglón $r$ (palabra):

    * Para cada documento $c$ (columna)

        * Si c tienen un valor mayor a $0$ en el renglón r, entonces para cada función hash $h_i$, si $h_i(r)$ es menor a $sig(i,c)$, entonces $sig(i,c)=hi(r)$.

Al final del algoritmo, $sig$ es la matriz de firmas de los documentos bajo las funciones hash $h_1,...,h_m$.

```
#include <Rcpp.h>
using namespace Rcpp;
// source this function into an R session using the Rcpp::sourceCpp 
// function (or via the Source button on the editor toolbar)

// [[Rcpp::export]]
int update_mat(NumericMatrix x, NumericVector doc, NumericVector h) {
   int nrow = x.nrow();
   int ncol = doc.size();
   for(int i =0; i < nrow; i++){
     	for(int j =0; j < ncol; j++){
       		if(h[i] < x( i,doc[j]-1)){
            	x(i,doc[j]-1) =  h[i];
            }
       }
   }
   return 1;
}
```
Ahora, el siguiente código ejecuta todo el algoritmo y crea la matriz de firmas.

```{r, eval=FALSE}
library(Rcpp)
sourceCpp('./update_mat.cpp')
minhash <- function(dat, hash.lista){
    n_words <- length(unique(enron$word_id))
    n_docs <- length(unique(enron$doc))
    p <- length(hash.lista)
    sig <- matrix(rep(Inf, p*n_docs) , ncol = n_docs)
    for(i in 1:n_words){
    #for(i in 1:10){
      hash_row <- sapply(hash.lista, function(h) h(i))
      document <- enron$doc[which(enron$word_id==i)]
      out <- update_mat(sig, document, hash_row)  
      print(i)
    }
    sig
}

firmas <- minhash(enron,hash.lista)
save(firmas, file='./Data/firmas.Rdata')
```

Encontramos ahora los mails similares al mail 900.

```{r}
load('./Data/firmas.Rdata')
doc.1.firma <- firmas[,900]
sim_est <- apply(firmas, 2, function(x){mean(x==doc.1.firma)})
docs.sim <- which(sim_est>=0.5)
docs.sim <- docs.sim[docs.sim!=900]
sim_est[(sim_est>=0.5 & sim_est<1)]
docs.sim
```
Podemos ver que los documentos que mayor similitud tienen al mail 900 tienen similitudes aproximadas de Jaccard de `r sim_est[(sim_est>=0.5 & sim_est<1)]`, y corresponden a los documentos `r docs.sim`.

Ya que redujimos los candidatos a documentos similares, podemos calcular las verdaderas similitudes de Jaccard sin miedo al tiempo pues solo hay que calcular `r length(docs.sim)` similitudes. Estas similitudes se pueden ver a continuación.

```{r, cache=TRUE}
similitudes <- sapply(docs.sim, function(i) jaccard(filter(enron, doc==900), filter(enron, doc==i)))
cbind(docs.sim, similitudes)
```

Podemos ver qué palabras son las que tienen en común cada mail similar con el mail 900.

```{r, cache=TRUE}
inter_palabras <- function(doc1, doc2){
  a <- intersect(doc1$word_id, doc2$word_id)
  as.character(vocab[a,1])
}
palabras <- sapply(docs.sim, function(i) inter_palabras(filter(enron, doc==900), filter(enron, doc==i)))
```
```{r}
palabras
```

El siguiente paso es encontrar, entre todos los posibles mails, pares de ellos que tienen alto grado de similitud. Aún cuando hemos reducido la dimensionalidad del problema, calcular todos los posibles pares de similitudes es infactible, por eso lo que haremos es usar *Locality-Sensitive Hashing* (LSH), el cual es una especie de *clustering* de datos; esto se logra asignando los documentos a una colección de cubetas de forma que documentos con alta similitud caen en la misma cubeta.

Utilizaremos 8 bandas de 2 hashes cada una para obtener pares candidatos para similitud y posteriormente devolver los 20 mejores candidatos para los documentos 100, 105 y 1400.

```{r, cache=TRUE}
#b=8; r=2;
b=20; r=10;
mat.split <- split(data.frame((firmas)), rep(1:b, each=r))
mat.hashed <- sapply(mat.split, function(mat){
    apply(mat, 2, function(x){sum(x)})
})
mat.hashed <- data.frame(mat.hashed)
mat.hashed$doc_no <- 1:nrow(mat.hashed)
# tab.1 <- table(mat.hashed[,1])
# codigos <- as.integer(names(tab.1[tab.1 >= 15]))
# length(codigos)
```
```{r, eval=FALSE}
candidatos <- lapply(1:b, function(i){
    tab.1 <- table(mat.hashed[,i])
    codigos <- as.integer(names(tab.1[tab.1 >= 2]))
    salida <- lapply(codigos, function(cod){ 
        mat.hashed$doc_no[mat.hashed[,i] == cod]
    })
   Reduce('cbind',lapply(salida, function(x){combn(x,2)}))
})
candidatos.tot <- t(Reduce('cbind',candidatos))
save(candidatos.tot, file='./Data/candidatos.tot.Rdata')
candidatos.tot.2 <- unique(candidatos.tot)
save(candidatos.tot.2, file='./Data/candidatos.tot.2.Rdata')
```
```{r}
load('./Data/candidatos.tot.2.Rdata')
```

Tenemos un total de `r dim(candidatos.tot.2)[1]` pares candidatos a similitud. Se pueden ver los primeros 6 pares a continuación.

```{r}
head(candidatos.tot.2)
dim(candidatos.tot.2)
```

Como nos interesan en particular los mails parecidos a los mails $100$, $105$ y $1400$; vemos cuántos pares candidatos hay para cada uno de ellos.

```{r}
mails <- c(100,105,1400)
sum(sapply(mails, function(i) sum(candidatos.tot.2==i)))
```

Para los pares candidatos vamos a calcular la similitud de Jaccard exacta.

```{r}
temp <- data.frame(candidatos.tot.2)
cand_100 <- filter(temp, (temp[,1]==100 | temp[,2]==100))
cand_105 <- filter(temp, (temp[,1]==105 | temp[,2]==105))
cand_1400 <- filter(temp, (temp[,1]==1400 | temp[,2]==1400))
rm(temp)

similitudes_100 <- sapply(1:nrow(cand_100), function(i) jaccard(filter(enron, doc==cand_100[i,1]), filter(enron, doc==cand_100[i,2])))
similitudes_105 <- sapply(1:nrow(cand_105), function(i) jaccard(filter(enron, doc==cand_105[i,1]), filter(enron, doc==cand_105[i,2])))
similitudes_1400 <- sapply(1:nrow(cand_1400), function(i) jaccard(filter(enron, doc==cand_1400[i,1]), filter(enron, doc==cand_1400[i,2])))
```

La similitud de Jaccard exacta para todos los pares candidatos se puede ver a continuación.

```{r}
arrange(cbind(cand_100,similitudes_100), desc(similitudes_100))
arrange(cbind(cand_105,similitudes_105), desc(similitudes_105))
arrange(cbind(cand_1400,similitudes_1400), desc(similitudes_1400))
```








